{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H82KsBSkblqQ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers nltk apache_beam rouge_score sacrebleu optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD7GLFPSqgQ2",
        "outputId": "976e6fe7-b6cf-4cc7-d2db-e0c74cf61413"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Computing metrics \n",
        "\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate import meteor_score\n",
        "import sacrebleu\n",
        "from nltk import word_tokenize \n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 't5-squad'\n",
        "model_dir = f\"gdrive/MyDrive/Models/{model_name}\"\n",
        "tokeniser = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "def compute_metrics(predictions_labels):\n",
        "    predictions, labels = predictions_labels\n",
        "    decoded_predictions = tokeniser.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = [rouge.score(ref, pred) for ref, pred in zip(decoded_labels, decoded_predictions)]\n",
        "    rouge1_f_scores = [score['rouge1'].fmeasure for score in rouge_scores]\n",
        "    rouge2_f_scores = [score['rouge2'].fmeasure for score in rouge_scores]\n",
        "    rougeL_f_scores = [score['rougeL'].fmeasure for score in rouge_scores]\n",
        "    result['rouge1'] = np.mean(rouge1_f_scores)\n",
        "    result['rouge2'] = np.mean(rouge2_f_scores)\n",
        "    result['rougeL'] = np.mean(rougeL_f_scores)\n",
        "\n",
        "    word_tokenized_predictions = [word_tokenize(pred) for pred in decoded_predictions]\n",
        "    word_tokenized_labels = [word_tokenize(label) for label in decoded_labels]\n",
        "    meteor_scores = [meteor_score.single_meteor_score(ref, pred) for ref, pred in zip(word_tokenized_labels, word_tokenized_predictions)]\n",
        "    result['meteor'] = np.mean(meteor_scores)\n",
        "  \n",
        "    label_sequence = [[label] for label in decoded_labels]\n",
        "    result['bleu'] = sacrebleu.corpus_bleu(decoded_predictions, label_sequence).score / 100\n",
        "    # Add average length of prediction \n",
        "    prediction_lens = [np.count_nonzero(pred != tokeniser.pad_token_id)\n",
        "                      for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing \n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 't5-squad'\n",
        "model_dir = f\"gdrive/MyDrive/Models/{model_name}\"\n",
        "tokeniser = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "def preprocess_squad(data):\n",
        "    inputs = ['summarize: ' + context.strip() for context in data['context']]\n",
        "    questions = [question for question in data['question']]\n",
        "    answers = [answer['text'][0] for answer in data['answers']]\n",
        "    labels = []\n",
        "    for i in range(len(questions)):\n",
        "        labels.append(questions[i] + '~' + answers[i])\n",
        "  \n",
        "    input_text = tokeniser(inputs, truncation=True, max_length=max_input_length)\n",
        "    target_text = tokeniser(labels, truncation=True, max_length=max_input_length)\n",
        "    return {'input_ids': input_text['input_ids'], 'attention_mask': input_text['attention_mask'], 'labels': target_text['input_ids']}\n",
        "  \n",
        "def preprocess_nq(data):\n",
        "    inputs = ['summarize: ' + context.strip() for context in data['context']]\n",
        "    questions = [question for question in data['question']]\n",
        "    answers = [answer for answer in data['answer']]\n",
        "    labels = []\n",
        "    for i in range(len(questions)):\n",
        "        labels.append(questions[i] + '~' + answers[i])\n",
        "  \n",
        "    input_text = tokeniser(inputs, truncation=True, max_length=max_input_length)\n",
        "    target_text = tokeniser(labels, truncation=True, max_length=max_input_length)\n",
        "    return {'input_ids': input_text['input_ids'], 'attention_mask': input_text['attention_mask'], 'labels': target_text['input_ids']}"
      ],
      "metadata": {
        "id": "3mRZLes23jHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qPOUvGo-FwRP"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset, load_metric, Dataset\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "dataset_name = 'squad'\n",
        "if dataset_name == 'squad':\n",
        "    model_name = 't5-squad'\n",
        "else:\n",
        "    model_name = 't5-nq'\n",
        "model_dir = f\"gdrive/MyDrive/Models/{model_name}\"\n",
        "\n",
        "model_checkpoint = 't5-base'\n",
        "tokeniser = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "max_input_length = 512\n",
        "\n",
        "if dataset_name == 'squad':\n",
        "    dataset = load_dataset('squad')\n",
        "    dataset_to_split = dataset['train']\n",
        "    train_dataset, val_dataset = train_test_split(dataset_to_split, test_size=0.2, random_state=42)\n",
        "    tokenised_dataset_train = Dataset.from_dict(train_dataset).map(preprocess_squad, batched=True)\n",
        "    tokenised_dataset_validation = Dataset.from_dict(val_dataset).map(preprocess_squad, batched=True)\n",
        "else:\n",
        "    with open('gdrive/MyDrive/Datasets/nq-train.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    features = {\n",
        "        \"context\": [item[\"context\"] for item in data],\n",
        "        \"question\": [item[\"question\"] for item in data],\n",
        "        \"answer\": [item[\"answer\"] for item in data],\n",
        "    }\n",
        "\n",
        "    dataset = Dataset.from_dict(features)\n",
        "    train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "    tokenised_dataset_train = Dataset.from_dict(train_dataset).map(preprocess_nq, batched=True)\n",
        "    tokenised_dataset_validation = Dataset.from_dict(val_dataset).map(preprocess_nq, batched=True)\n",
        "\n",
        "def create_t5_trainer(trial, tokenised_dataset_train, tokenised_dataset_validation):\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=model_dir,\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        evaluation_strategy='epoch',\n",
        "        logging_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        learning_rate=3e-5,\n",
        "        weight_decay=0.01,\n",
        "       fp16=True,\n",
        "       predict_with_generate=True,\n",
        "    )\n",
        "\n",
        "  data_collator = DataCollatorForSeq2Seq(tokeniser)\n",
        "\n",
        "  trainer = Seq2SeqTrainer(\n",
        "    model=AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint),\n",
        "    args=training_args,\n",
        "    train_dataset=tokenised_dataset_train,\n",
        "    eval_dataset=tokenised_dataset_validation,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokeniser\n",
        "    compute_metrics=compute_metrics\n",
        "  )\n",
        "\n",
        "  return trainer\n",
        "\n",
        "trainer = create_t5_trainer(trial, tokenised_dataset_train, tokenised_dataset_validation)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset, Dataset\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import json\n",
        "\n",
        "dataset_name = 'squad'\n",
        "if dataset_name == 'squad':\n",
        "    model_name = 't5-squad'\n",
        "else:\n",
        "    model_name = 't5-nq'\n",
        "  \n",
        "model_dir = f\"gdrive/MyDrive/Models/{model_name}\"\n",
        "tokeniser = AutoTokenizer.from_pretrained(model_dir)\n",
        "max_input_length = 512 \n",
        "\n",
        "if dataset_name == 'squad':\n",
        "    dataset = load_dataset('squad')\n",
        "    test_dataset = dataset['validation']\n",
        "    tokenised_dataset_test = test_dataset.map(preprocess_squad, batched=True)\n",
        "else:\n",
        "    with open('gdrive/MyDrive/Datasets/nq-dev.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    features = {\n",
        "        \"context\": [item[\"context\"] for item in data],\n",
        "        \"question\": [item[\"question\"] for item in data],\n",
        "        \"answer\": [item[\"answer\"] for item in data],\n",
        "    }\n",
        "\n",
        "    test_dataset = Dataset.from_dict(features)\n",
        "    tokenised_dataset_test = test_dataset.map(preprocess_nq, batched=True)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(tokenised_dataset_test, batch_size=16)\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "for batch in enumerate(dataloader):\n",
        "    input_ids = batch[\"input_ids\"].squeeze()\n",
        "    label_ids = batch[\"labels\"].squeeze()\n",
        "  \n",
        "    with torch.no_grad():\n",
        "        predictions = model.generate(input_ids, num_beams=8, max_length=64)\n",
        "\n",
        "    all_predictions += predictions\n",
        "    all_labels += labels\n",
        "\n",
        "predictions_labels = [all_predictions, all_labels]\n",
        "metrics = compute_metrics(predictions_labels)\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "U-Dn7cMvztON"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}