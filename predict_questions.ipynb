{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers numpy allennlp nltk pke scipy networkx"
      ],
      "metadata": {
        "id": "Y9CzvY3ut67T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "model_dir = f\"gdrive/MyDrive/<PATH TO MODEL>\""
      ],
      "metadata": {
        "id": "jV3tmQowuQi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "from nltk.tree import Tree\n",
        "from nltk.corpus import stopwords\n",
        "import pke\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "class Predictor:\n",
        "    # SET T5 MODEL DIRECTORY HERE\n",
        "    model_dir = f\"gdrive/MyDrive/<PATH TO MODEL>\"\n",
        "    T5Tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    T5Model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
        "    constituency_predictor = Predictor.from_path(\n",
        "        \"https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\")\n",
        "    GPT2Tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    GPT2Model = TFGPT2LMHeadModel.from_pretrained(\n",
        "        \"gpt2\", pad_token_id=GPT2Tokenizer.eos_token_id)\n",
        "\n",
        "    def predict_sa(self, sentence, num_return_sequences):\n",
        "        predicted_questions = []\n",
        "\n",
        "        inputs = [\"summarize: \" + sentence]\n",
        "        inputs = self.T5Tokenizer(inputs, max_length=512,\n",
        "                                  truncation=True, return_tensors=\"pt\")\n",
        "        outputs = self.T5Model.generate(**inputs, num_beams=15, temperature=0.8,\n",
        "                                        num_return_sequences=num_return_sequences,\n",
        "                                        early_stopping=True, min_length=10, max_length=64)\n",
        "        for output in outputs:\n",
        "            decoded_output = self.T5Tokenizer.decode(\n",
        "                output, skip_special_tokens=True\n",
        "            )\n",
        "            predicted_question = nltk.sent_tokenize(decoded_output.strip())[0]\n",
        "            question_answer = predicted_question.split('?')\n",
        "            # If not exactly one question mark discard question, something is malformed\n",
        "            if len(question_answer) == 2:\n",
        "                predicted_questions.append(\n",
        "                    {'type': 'SA', 'question': question_answer[0] + '?', 'answer': question_answer[1]})\n",
        "\n",
        "        return predicted_questions\n",
        "\n",
        "    def predict_mcq(self, question, answer):\n",
        "        synsets = wn.synsets(answer, 'n')\n",
        "        if not synsets:\n",
        "            return None\n",
        "        synset = synsets[0]\n",
        "        hypernyms = synset.hypernyms()\n",
        "        distractors = []\n",
        "\n",
        "        for hypernym in hypernyms:\n",
        "            hyponyms = hypernym.hyponyms()\n",
        "            for hyponym in hyponyms:\n",
        "                try:\n",
        "                    distractor = hyponym.lemma_names()[0]\n",
        "                except IndexError:\n",
        "                    return None\n",
        "                # Deal with multi-word distractors\n",
        "                distractor = distractor.replace('_', ' ')\n",
        "\n",
        "                # Avoid duplicate options\n",
        "                if distractor != answer and distractor not in distractors:\n",
        "                    distractors.append(distractor.title())\n",
        "\n",
        "        # Always need at least 3 distractors\n",
        "        if len(distractors) < 3:\n",
        "            return None\n",
        "\n",
        "        mcq_answers = [answer]\n",
        "        random.shuffle(distractors)\n",
        "        for word in distractors[:3]:\n",
        "            mcq_answers.append(word)\n",
        "        random.shuffle(mcq_answers)\n",
        "\n",
        "        # Convert to string for DB\n",
        "        final_answer = ''\n",
        "        for option in mcq_answers:\n",
        "            final_answer += option.capitalize() + '|'\n",
        "        # Add on real answer at end\n",
        "        final_answer += answer.capitalize()\n",
        "\n",
        "        return {'type': 'MCQ', 'question': question, 'answer': final_answer}\n",
        "\n",
        "    def predict_fib(self, text):\n",
        "        def get_keyphrases(text):\n",
        "            try:\n",
        "                stoplist = stopwords.words('english')\n",
        "                text = ' '.join([word for word in text.split()\n",
        "                                if word.lower() not in stoplist])\n",
        "\n",
        "                extractor = pke.unsupervised.MultipartiteRank()\n",
        "                extractor.load_document(input=text, language='en')\n",
        "                extractor.candidate_selection()\n",
        "                extractor.candidate_weighting()\n",
        "                keyphrases = extractor.get_n_best(n=15)\n",
        "                return keyphrases\n",
        "            except Exception as e:\n",
        "                return None\n",
        "\n",
        "        def get_keyphrase_sentence_mapping(keyphrases, sentences):\n",
        "            keyphrase_sentence_mapping = {}\n",
        "            for item in keyphrases:\n",
        "                # Each keyphrase is a tuple of (actual_keyphrase, weighting)\n",
        "                keyphrase = item[0]\n",
        "                keyphrase_sentence_mapping[keyphrase] = []\n",
        "                for sentence in sentences:\n",
        "                    if keyphrase in sentence:\n",
        "                        keyphrase_sentence_mapping[keyphrase].append(sentence)\n",
        "\n",
        "            return keyphrase_sentence_mapping\n",
        "\n",
        "        keyphrases = get_keyphrases(text)\n",
        "        if not keyphrases:\n",
        "            return None\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        keyphrase_sentence_mapping = get_keyphrase_sentence_mapping(\n",
        "            keyphrases, sentences)\n",
        "        output_questions = []\n",
        "\n",
        "        for keyphrase in keyphrase_sentence_mapping.keys():\n",
        "            for sentence in keyphrase_sentence_mapping[keyphrase]:\n",
        "                if sentence.count(keyphrase) == 1:\n",
        "                    sentence_with_blank = sentence.replace(keyphrase, '______')\n",
        "                    output_questions.append(\n",
        "                        {'type': 'FIB', 'question': 'Fill in the blank in the following: ' + sentence_with_blank, 'answer': keyphrase})\n",
        "\n",
        "        return output_questions\n",
        "\n",
        "    def predict_tf(self, sentence):\n",
        "        def find_rightmost_vp_np(tree, rightmost_vp=None, rightmost_np=None):\n",
        "            # Check whether node is a leaf\n",
        "            if isinstance(tree, Tree):\n",
        "                subtree = tree[-1]\n",
        "                if isinstance(subtree, Tree):\n",
        "                    if subtree.label() == 'NP':\n",
        "                        rightmost_vp = subtree\n",
        "                    elif subtree.label() == 'VP':\n",
        "                        rightmost_np = subtree\n",
        "                    return find_rightmost_vp_np(subtree, rightmost_vp, rightmost_np)\n",
        "                else:\n",
        "                    return rightmost_vp, rightmost_np\n",
        "            else:\n",
        "                return rightmost_vp, rightmost_np\n",
        "\n",
        "        def get_cutoff_sentence(sentence, ending):\n",
        "            sentence_split = sentence.split()\n",
        "            ending_split = ending.split()\n",
        "            try:\n",
        "                cutoff_sentence_split = sentence_split[:len(\n",
        "                    sentence_split)-len(ending_split)]\n",
        "            except IndexError:\n",
        "                return None\n",
        "            return ' '.join(cutoff_sentence_split)\n",
        "\n",
        "        try:\n",
        "            true_false = [\"True\", \"False\"]\n",
        "            true_or_false = random.choice(true_false)\n",
        "            if true_or_false == \"True\":\n",
        "                return {'type': 'TF', 'question': 'True or False? ' + sentence, 'answer': 'True'}\n",
        "\n",
        "            # Strip trailing punctuation\n",
        "            sentence = sentence.rstrip('?:!.,;')\n",
        "            parse_tree = Tree.fromstring(\n",
        "                self.constituency_predictor.predict(sentence=sentence)['trees'])\n",
        "\n",
        "            rightmost_vp, rightmost_np = find_rightmost_vp_np(parse_tree)\n",
        "            if not rightmost_vp and not rightmost_np:\n",
        "                return None\n",
        "            elif not rightmost_vp:\n",
        "                sentence_ending = ' '.join(rightmost_np.leaves())\n",
        "            elif not rightmost_np:\n",
        "                sentence_ending = ' '.join(rightmost_vp.leaves())\n",
        "            else:\n",
        "                sentence_ending_vp = ' '.join(rightmost_vp.leaves())\n",
        "                sentence_ending_np = ' '.join(rightmost_np.leaves())\n",
        "                sentence_ending = max(sentence_ending_vp,\n",
        "                                      sentence_ending_np, key=len)\n",
        "\n",
        "            cutoff_sentence = get_cutoff_sentence(sentence, sentence_ending)\n",
        "            if not cutoff_sentence:\n",
        "                return None\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "        input_ids = self.GPT2Tokenizer.encode(\n",
        "            cutoff_sentence, return_tensors='tf')\n",
        "\n",
        "        outputs = self.GPT2Model.generate(\n",
        "            input_ids,\n",
        "            do_sample=True,\n",
        "            max_length=100,\n",
        "            no_repeat_ngram_size=2,\n",
        "            repetition_penalty=10.0,\n",
        "            temperature=0.8,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "\n",
        "        generated_sentences = []\n",
        "        for output in outputs:\n",
        "            decoded_sentence = self.GPT2Tokenizer.decode(\n",
        "                output, skip_special_tokens=True)\n",
        "            final_sentence = nltk.sent_tokenize(decoded_sentence)[0]\n",
        "            generated_sentences.append(final_sentence)\n",
        "\n",
        "        false_sentence = random.choice(generated_sentences)\n",
        "        return {'type': 'TF', 'question': 'True or False? ' + false_sentence, 'answer': 'False'}\n",
        "\n",
        "    def chunk_text(self, text, chunk_size):\n",
        "        chunks = []\n",
        "        words = text.split()\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for word in words:\n",
        "            if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
        "                current_chunk += word + \" \"\n",
        "            else:\n",
        "                chunks.append(current_chunk)\n",
        "                current_chunk = word + \" \"\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk)\n",
        "\n",
        "        return chunks\n",
        "    \n",
        "    def predict_sa_mcq(self, text, predict_mcq):\n",
        "\n",
        "        def chunk_text(text, chunk_size):\n",
        "            chunks = []\n",
        "            words = text.split()\n",
        "            current_chunk = \"\"\n",
        "\n",
        "            for word in words:\n",
        "                if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
        "                    current_chunk += word + \" \"\n",
        "                else:\n",
        "                    chunks.append(current_chunk)\n",
        "                    current_chunk = word + \" \"\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "\n",
        "            return chunks\n",
        "\n",
        "        output_questions = []\n",
        "        \n",
        "        chunked_text = chunk_text(text, 256)\n",
        "        vectoriser = TfidfVectorizer()\n",
        "        for chunk in chunked_text:\n",
        "            no_sentences = len(nltk.sent_tokenize(chunk))\n",
        "            predicted_questions_sa = self.predict_sa(chunk, no_sentences + 2)\n",
        "\n",
        "            if predicted_questions_sa:\n",
        "                # Don't include '?' in sentence\n",
        "                question_vectors = vectoriser.fit_transform([\n",
        "                    question['question'][:-1] for question in predicted_questions_sa])\n",
        "                answer_vectors = vectoriser.fit_transform([\n",
        "                    question['answer'][:-1] for question in predicted_questions_sa])\n",
        "                question_similarity_matrix = cosine_similarity(\n",
        "                    question_vectors)\n",
        "                answer_similarity_matrix = cosine_similarity(answer_vectors)\n",
        "\n",
        "                remove_list = []\n",
        "                # Remove questions too similar to each other\n",
        "                for i in range(len(predicted_questions_sa)):\n",
        "                    if i not in remove_list:\n",
        "                        for j in range(i+1, len(predicted_questions_sa)):\n",
        "                            if question_similarity_matrix[i][j] > 0.7 or answer_similarity_matrix[i][j] > 0.9:\n",
        "                                remove_list.append(j)\n",
        "\n",
        "                sa_questions = [\n",
        "                    predicted_questions_sa[i] for i in range(len(predicted_questions_sa)) if i not in remove_list]\n",
        "\n",
        "                # Now try to generate MCQs if enabled\n",
        "                if predict_mcq:\n",
        "                    for question in sa_questions:\n",
        "                        # If only one word in answer we can generate an MCQ,\n",
        "                        # otherwise leave as it is\n",
        "                        if len(' '.split(question['answer'])) == 1:\n",
        "                            predicted_question_mcq = self.predict_mcq(\n",
        "                                question['question'], question['answer'])\n",
        "                            if predicted_question_mcq:\n",
        "                                output_questions.append(predicted_question_mcq)\n",
        "                            else:\n",
        "                                output_questions.append(question)\n",
        "                        else:\n",
        "                            output_questions.append(question)\n",
        "        \n",
        "        return output_questions \n",
        "\n",
        "    def predict(self, text):\n",
        "        predicted_questions = []\n",
        "\n",
        "        # FIB questions\n",
        "        fib_questions = self.predict_fib(text)\n",
        "        if fib_questions:\n",
        "            predicted_questions += fib_questions\n",
        "\n",
        "        # SA and MCQ questions\n",
        "        sa_mcq_questions = self.predict_sa_mcq(text, True)\n",
        "        if sa_mcq_questions:\n",
        "            predicted_questions += sa_mcq_questions\n",
        "\n",
        "        # TF questions\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        for sent in sentences:\n",
        "            predicted_question_tf = self.predict_tf(sent)\n",
        "            if predicted_question_tf:\n",
        "                predicted_questions.append(predicted_question_tf)\n",
        "\n",
        "        return predicted_questions"
      ],
      "metadata": {
        "id": "cwHQt8Q7tv_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = Predictor() \n",
        "text = \"YOUR TEXT TO GENERATE QUESTIONS FROM HERE\"\n",
        "questions = predictor.predict(text)\n",
        "print(questions)"
      ],
      "metadata": {
        "id": "zI0S3Q2utz1T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}